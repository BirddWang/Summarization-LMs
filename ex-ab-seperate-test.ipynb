{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data (100 sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from secret import OPENAI_API_KEY\n",
    "client = OpenAI(api_key= OPENAI_API_KEY)\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mname = 'gpt-3.5-turbo'\n",
    "temperature = 0\n",
    "max_token = 1024\n",
    "prefix_prompt = \"\"\"Given a document, do the followling tasks:\n",
    "(1) According to the document, find at least 3 important events.\n",
    "(2) With the retrieved event, compose a summary in 3 sentences.\n",
    "\n",
    "Example:\n",
    "============Example============\n",
    "Prompt:\n",
    "Document: [document]\n",
    "Update:\n",
    "Important Events:\n",
    "1. [EVENT_1]\n",
    "2. [EVENT_2]\n",
    "3. [EVENT_3]\n",
    "...\n",
    "\n",
    "Summary:\n",
    "[summary]\n",
    "===============================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _response_process(content: str, document: str):\n",
    "    event = content.split(\"Important Events:\\n\")[1].split(\"Summary\")[0]\n",
    "    eventlog = event.split(\"\\n\")\n",
    "    rationale = \"\"\n",
    "    for e in eventlog:\n",
    "        if len(e) == 0: continue\n",
    "        rationale += re.sub(r'^\\d+\\. ', '', e)\n",
    "\n",
    "    summary = content.split(\"Summary:\\n\")[1]\n",
    "\n",
    "    result = {\n",
    "        \"article\": document,\n",
    "        \"rationale\": rationale,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _store_as_jsonl(results: list):\n",
    "    with open(\"data/cnndm/rationale.jsonl\", mode=\"a\") as f:\n",
    "        for r in results:\n",
    "            r = json.dumps(r)\n",
    "            f.write(r + \"\\n\")\n",
    "        f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_api(data_size: int):\n",
    "    document = []\n",
    "    gt_summary = []\n",
    "    results = []\n",
    "    with open(\"data/cnndm/train.jsonl\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "        for i in range(100):\n",
    "            document.append(data[i][\"article\"])\n",
    "            gt_summary.append(data[i][\"highlights\"])\n",
    "        f.close()\n",
    "    for i in range(data_size):\n",
    "        prompt = f\"{prefix_prompt}\\nPrompt:\\n[Document]: {document[i]}\\n\\nUpdate:\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=mname,\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_token\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "        results.append(_response_process(response.choices[0].message.content, document[i]))\n",
    "        _store_as_jsonl(results)\n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/Summarization-LMs/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目標：利用兩個bart當作extractor & abstractor，看看有沒有能力生出更好的Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "from datasets import load_dataset\n",
    "class CNNDMDataset(Dataset):\n",
    "    def __init__(self, data, max_len:int=1024, data_len:int=1000, ):\n",
    "        #input type [article, highlights, id]\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tok = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "        self.max_len = max_len\n",
    "        self.data_len = data_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tok.encode_plus(self.data[idx]['article'], max_length=self.max_len, return_tensors='pt', truncation=True, padding='max_length')\n",
    "        tgt = self.tok.encode_plus(self.data[idx]['summary'], max_length=self.max_len, return_tensors='pt', truncation=True, padding='max_length')\n",
    "        ral = self.tok.encode_plus(self.data[idx]['rationale'], max_length=self.max_len, return_tensors='pt', truncation=True, padding='max_length')\n",
    "        src_input_ids = src['input_ids'].squeeze()\n",
    "        tgt_input_ids = tgt['input_ids'].squeeze()\n",
    "        ral_input_ids = ral['input_ids'].squeeze()\n",
    "\n",
    "        result = {\n",
    "            'src_input_ids': src_input_ids,\n",
    "            'tgt_input_ids': tgt_input_ids,\n",
    "            'ral_input_ids': ral_input_ids,\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('data/cnndm/rationale.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f.readlines()]\n",
    "dataset = CNNDMDataset(data, data_len=100)\n",
    "train_loader = DataLoader(dataset, batch_sampler=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "abstractor_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    epoch = 3\n",
    "    accumulate_count = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    model.to(device)\n",
    "    for (i, batch) in enumerate(train_loader):\n",
    "        input_ids = batch['src_input_ids'].to(device)\n",
    "        tgt_ids = batch['tgt_input_ids'].to(device)\n",
    "        ral_ids = batch['ral_input_ids'].to(device)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
